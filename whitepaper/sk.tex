% This file is part of the SaveKepler project.
% Copyright 2013 the authors.

\documentclass[letterpaper,12pt,preprint]{aastex}
\usepackage{enumitem, color}
\definecolor{hypercolor}{RGB}{0,0,127}
\usepackage[%
  citecolor=hypercolor,%
  linkcolor=hypercolor,%
  urlcolor=hypercolor,%
  backref=false,%
  pagebackref=false%
]{hyperref}%

\newcommand{\sectionname}{Section}
\newcommand{\documentname}{\textsl{white paper}}
\newcommand{\foreign}[1]{\textit{#1}}
\newcommand{\vs}{\foreign{vs}}
\newcommand{\etal}{\foreign{et~al.}}
\newcommand{\observatory}[1]{\textsl{#1}}
\newcommand{\Kepler}{\observatory{Kepler}}
\newcommand{\TESS}{\observatory{TESS}}
\newcommand{\SDSS}{\observatory{SDSS}}
\newcommand{\WISE}{\observatory{WISE}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\MAST}{\project{MAST}}
\newcommand{\TheTractor}{\project{The~Tractor}}
\newcommand{\emcee}{\project{emcee}}
\newcounter{inlineitem}
\setcounter{inlineitem}{0}
\newcommand{\inlineitem}{\refstepcounter{inlineitem}\textbf{\textsl{(\theinlineitem)}}}
\newcounter{address}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\begin{document}\sloppy\sloppypar\thispagestyle{empty}

\title{Maximizing \Kepler\ science return per telemetered pixel: \\
  Detailed models of the focal plane in the two-wheel era\altaffilmark{\ref{kcall}}}

\author{%
  David~W.~Hogg\altaffilmark{\ref{CCPP},\ref{MPIA}},
  Ruth~Angus\altaffilmark{\ref{Oxford}},
  Tom~Barclay\altaffilmark{\ref{Ames}},
  Rebekah~Dawson\altaffilmark{\ref{CfA}},
  Rob~Fergus\altaffilmark{\ref{Courant}},
  Dan~Foreman-Mackey\altaffilmark{\ref{CCPP}},
  Stefan Harmeling\altaffilmark{\ref{MPIIS}},
  Michael~Hirsch\altaffilmark{\ref{UCL}, \ref{MPIIS}},
  Dustin~Lang\altaffilmark{\ref{CMU}},
  Benjamin~T.~Montet\altaffilmark{\ref{Caltech}},
  David~Schiminovich\altaffilmark{\ref{Columbia}} \&
  Bernhard~Sch\"olkopf\altaffilmark{\ref{MPIIS}}%
}

\setcounter{address}{1}
\altaffiltext{\theaddress}{\stepcounter{address}\label{kcall}%
  A white paper submitted in response to the \Kepler\ Project Office
  \textit{Call for White Papers: Soliciting Community Input for
    Alternate Science Investigations for the Kepler Spacecraft}
  released 2013 August 02
  (\url{http://keplergo.arc.nasa.gov/docs/Kepler-2wheels-call-1.pdf})}
\altaffiltext{\theaddress}{\stepcounter{address}\label{CCPP}%
  Center for Cosmology and Particle Physics, Department of Physics, New York University}
\altaffiltext{\theaddress}{\stepcounter{address}\label{MPIA}%
  Max-Planck-Institut f\"ur Astronomie, Heidelberg, Germany}
\altaffiltext{\theaddress}{\stepcounter{address}\label{Oxford}%
  Department of Physics, Oxford University}
\altaffiltext{\theaddress}{\stepcounter{address}\label{Ames}%
  NASA Ames Research Center}
\altaffiltext{\theaddress}{\stepcounter{address}\label{CfA}%
  Havard--Smithsonian Center for Astrophysics}
\altaffiltext{\theaddress}{\stepcounter{address}\label{Courant}%
  Courant Institute of Mathematical Sciences, New York University}
\altaffiltext{\theaddress}{\stepcounter{address}\label{UCL}%
  Department of Physics and Astronomy, University College London}
\altaffiltext{\theaddress}{\stepcounter{address}\label{MPIIS}%
  Max-Planck-Institut f\"ur Intelligente Systeme, T\"ubingen}
\altaffiltext{\theaddress}{\stepcounter{address}\label{CMU}%
  McWilliams Center for Cosmology, Carnegie Mellon University}
\altaffiltext{\theaddress}{\stepcounter{address}\label{Caltech}%
  Department of Astronomy, California Institute of Technology}
\altaffiltext{\theaddress}{\stepcounter{address}\label{Columbia}%
  Department of Astrophysics, Columbia University}

~
\clearpage

\paragraph{executive summary:}
\inlineitem~Fundamentally, the \textbf{primary recommendation} of this \documentname\
  is \emph{image modeling}%
  ---building a detailed model of pixel sensitivities
  (including possibly intra-pixel sensitivity maps),
  point-spread function,
  and sky, bias, and dark signals,
  all as a function of focal-plane position%
  ---along with a model of the position and brightness of every star in the field.
This level of modeling has not happened with \Kepler\ up to now
  because the data have been made extremely precise with good pointing
  and aperture photometry.
In the two-wheel era, \Kepler\ will not maintain precise pointing.
This is a blessing as well as a curse:
It reduces the precision of naive aperture photometry,
  but it provides data diversity that permits independent inference
  of the sensitivity map and the point-spread function.
We propose capitalizing on this to
  \textbf{develop a probabilistic generative model of the \Kepler\ pixels.}
We argue that this modeling may permit continuance of photometry at 10-ppm-level precision.
We demonstrate baby steps towards focal-plane models
  along two different directions:
In one, the model is physical;
  its parameters are parameters of the spacecraft, detectors, and optics.
In the other, the model is data-driven but motivated by ideas of causality;
  in this case the parameters control the ways
  disparate data sources can be used to predict one another.
We demonstrate that the expected drift or jitter in positions in the two-weel era
  will \textsl{(a)}~help with constraining these kinds of model,
  and \textsl{(b)}~be obviated (in terms of loss of precision) by such modeling.
These results are relevant to \emph{almost any} scientific goal for the repurposed mission.
We have several secondary recommendations:

\inlineitem~If \Kepler\ continues to observe the current \Kepler\ field,
  we would recommend shortening exposure times to about 5\,min, and increasing cadence,
  to reduce the effect of pointing drift and increase the sensitivity of the models.
If \Kepler\ switches to observing a set of new but specially chosen ``low-torque'' fields,
  it could continue with its current exposure times.
The telemetry burden of any increase in cadence can be offset
  by reducing the number of target stars.

\inlineitem~It will be imperative to operate the spacecraft
  with at least one of the following modifications to operational software:
Either the pointing will need to be adjusted frequently using
  the two operational wheels and some propellant;
  or else the telemetered focal-plane apertures will have to be enlarged;
  or else the apertures will have to be adapted in real time to follow drifting stars.

\inlineitem~It will also be wise to further capitalize on the ``blessing'' of the drift
  by diversifying the observations in other ways:
Deliberate focus pulls, dithers, and integration-time adjustments,
  either throughout the mission
  or else in specified ``calibration periods''
  will provide much more data support for hard-to-constrain model parameters.
These calibration observations will not only improve the models of the two-wheel data,
  they have the promise to substantially improve the precision of our understanding
  of the extant \Kepler\ data that we expect contains many latent and undiscovered signals.

\inlineitem~Whether the two-wheel survey strategy ends up being
  to continue observations in the current \Kepler\ field,
  or to work in the low-torque fields%
  ---we discuss scientific suvey strategies in a companion \documentname\ (by Montet et al.)---%
  it is our view that \Kepler\ can still accomplish
  what many of the present authors consider its key scientific goal,
  which is \textbf{to find Earth-like planets on year-ish orbits around Sun-like stars}.
With a multi-pronged image-modeling effort and
  a bit of good luck
  (with respect to assumptions about spacecraft hardware and data-modeling software),
  it is our view that a two-wheeled \Kepler\ can still
  be used to pursue this deep and important mission.
That said, what's written in this \documentname\ is fundamentally
  agnostic about the scientific program in the two-wheel era.

\clearpage

\section{Philosophy and motivation}

The \Kepler\ satellite%
  ---like other very precise time-domain photometric projects---%
  achieves its precision by a combination of two principles:
The first is to make a telescope and camera that is as stable and controlled as possible.
The second is to make each observation as identical as possible to all previous observations.
The great success of this joint strategy is evident in \Kepler's discoveries, to wit,
  enormous numbers of planets of a wide range of sizes and periods.

In the two-wheel era, it will not be possible
  to make each observation as identical to the fiducial observation
  as it was in the three- and four-wheel eras (HOGG CITE TECH DOCUMENTS).
However, the fact that the telescope and camera were so stable
  (plus perhaps some appeal to the determinism of the macroscopic laws of physics)
  implies that it must be possible to build a deterministic, predictive model
  of the imager that can account for pointing changes or drift over time.
One way of stating it is this:
Bad spacecraft (s/c) pointing leads to the existence of
  three unknown functions of time
  (which can be thought of as the three solid-body Euler angles).
These three Euler-angle functions affect%
  ---in a predictable way---%
  every single one of the hundreds of thousands of detectable stars
  in the camera field of view,
  each of which affects the intensity field touching a few to many pixels.
There is an \emph{overwhelming abundance} of information in the data
  to constrain these three functions,
  and even \emph{more} information if the particular functions
  are different for every exposure (as they are expected to be).

While it is obvious \emph{in principle} that we can infer
  the intra-exposure s/c pointing drift and jitter history,%
  \footnote{Indeed inference of non-trivial intra-exposure
    camera pointing trajectories has been demonstrated in the far more difficult
    case of single-exposure natural images (\cite{whyte2010,hirsch2011}).
  That is, the three-axis history of camera orientation path
    can be reliably inferred in an individual image,
    without even multiple exposures to use
    and no knowledge of the details of the ``true scene''.
  Relative to these situations, the \Kepler\ problem is a walk in the park!
  That said, the precision requirements of \Kepler\ science
    are much more challenging than those of natural image de-blurring.}
  it is not yet obvious that such inference will directly lead to very precise photometry.
After all, recovery of precise photometry from a source
  that is moving across the focal plane
  requires knowledge of the device geometry and sensitivity
  (flat-field) at the pixel level,
  along with the point-spread function (PSF) and its variations with time and position.
When---as with \Kepler---all data have been taken at rigidly fixed pointing,
  it is hard (or maybe impossible) to obtain independent data-supported models of
  the PSF and the flat-field,
  let alone any intra-pixel sensitivity variations
  or pixel-level additive signals (like bias or dark currents).
That is, 
  \Kepler\ data are taken in a mode that is
  near-pessimal%
  \footnote{Neologism ``pessimal'' is the antonym of ``optimal''.}
  for the determination of calibration parameters.

The death of \Kepler's second wheel (taking it down to two)
  occurred at a tragic moment:
The mission is on track to discover Earth-like planets
  orbiting Sun-like stars on year-ish (habitable-zone) orbits,
  and for such targets, five or six years is a lot better than four.
Indeed, when signals are sparse, significance
  (as estimated by, say, a p-value)
  can grow much faster than any small power of mission lifetime.%
  \footnote{Shouts out to Ben Weiner (Arizona) for this insight.}
It is our view that \emph{\Kepler\ core goals remain outrageously interesting}.
With this in mind, we advocate using \emph{methods of image modeling} to retain
  or approximate three-wheel photometric precision
  in an era of two-wheel pointing degradation.
If that is possible---and we think it will be---we would recommend continuing work
  along the direction of \Kepler's core mission,
  which (in our view) is to find Earth analogs and other habitable-zone oceany rocks.

Of course the two-wheel mode will bring other challenges, like
  possible hard constraints on field choice,
  changes to exposure time or cadence,
  and changes to flight software that selects pixel regions for downlink.
For all these reasons, we might have to observe fewer stars in the two-wheel era.
Since \Kepler's mission so far has taught us a lot about the variability
  properties of stars,
  the prevalence of rocky planets around different kinds of stars,
  and the brightness limits of transit searching at different transit depths and durations,
  it is our view that it will be possible to design a program that will deliver
  large payloads of new and important planetary systems,
  and all relevant to the Earth-analog question.
We present these scientific proposals in a separate, companion white paper (Montet \etal),
  because although we love eta-Earth,
  \emph{what we say here about image modeling is equally applicable
  to any two-wheel repurpose} for the \Kepler\ satellite.

Right now, all of \Kepler's spine-tingling precision
  has been obtained by peforming rigid integer-pixel aperture photometry
  on flat-fielded and background-subtracted image data.
Furthermore, only something like half of the downlinked pixels are used in the apertures,
  and every pixel that \emph{is} used is used with identical weight.
That is, there is no ``optimal extraction'' or ``PSF photometry'' being
  used to combine the corrected pixel data;
  the PSF model is used only to determine optimized photometric apertures.
Another motivation for and value of the work we present here will be
  in the exploitation of the \emph{existing} data.
In some sense, one of the proposals we are making is to use the two-wheel era
  to generate the calibration data we need to fully exploit the existing data.
Again, this calibration activity could bring enormous scientific value to the \Kepler\ community
  no matter what the repurposed mission.

One final point of philosophy:
All of the code used to make the demonstrations in this \documentname\ is available
  in open-source or at least publicly readable repositories on the web.%
\footnote{Our toy future-data simulation---and code for physical modeling thereof---is at
  \url{http://astrometry.net/svn/trunk/projects/kepler/};
  it is a sub-project of our general image-modeling framework
  called ``\TheTractor'' (\citealt{hoggtractor}, \citealt{langtractor}), which is at
  \url{http://theTractor.org/}.
  Our data-driven modeling code is at
  \url{https://github.com/dfm/causal-kepler}.
  Code to build physical models of the pixels in the extant \Kepler\ data is at
  \url{https://github.com/dfm/kpsf}.
  Our Python wrapping of the \MAST\ interface to the \Kepler\ data is at
  \url{https://github.com/dfm/kplr},
  and most of our probabilistic inference involves ensemble sampling
  with \emcee\ (\citealt{emcee}), which is at
  \url{http://dan.iel.fm/emcee/}.}
We encourage the \Kepler\ community to fork, build on, and contribute back to these code bases.

\section{Data-driven modeling}\label{sec:datadriven}

DFM:  HERE EITHER YOU OR I COULD PUT A SEGUE THAT SAYS
  ``MODELING CAN BE PHYSICAL OR DATA-DRIVEN''
  and WE ARE STARTING WITH DATA-DRIVEN.
ALSO MAYBE INTRODUCE TRAIN, VALID, TEST?

\subsection{Predicting pixels with pixels}\label{sec:pixels}

One example of data-driven modeling is the set of methods that
the core Kepler science team has developed for removing some
systematic instrumental signals from the extracted light curves.
The set of algorithms called PDC \citep{map-pdc1,map-pdc2} are particularly
successful because they remove instrumental effects (assumed common between
nearby targets on the detector) while retaining astrophysical signals (assumed
unique to the target of interest).
These methods use a data-driven linear model built directly from the aperture
photometry of nearby stars; that is, they operate on coadded pixel values, not individual pixels.

Here, we present a demonstration in the form of an extension to this model that takes advantage of the causal
structure of the problem (CITE SOMETHING).
We also argue that this procedure is best performed \emph{at the pixel level}
instead of on the extracted photometric time series.
To start, we describe a model to remove systematic effects from the time
series in a single pixel using the fluxes of pixels from some $K\ge1$ nearby
star(s) and the ``historical'' time series of the target pixel.
The main insight of this method is that the systematic effects are actually
\emph{causally predictable}.
In other words, the (recent) history of the detector sensitivity,
as encoded in large numbers of pixel histories, provides a
better estimate of the conditions at a given moment than an
instantaneous measurement considered alone.

\paragraph{Implementation}
In this section,
we will discuss the calibration of pixel $i$ of target star $n$ at time $t_k$
using the pixels $j$ on target $m \ne n$ in the sliding window $t_k - \delta t
\le t \le t_k + \delta t$.
As an extension, we can also use the data in pixels $i^\prime$ from the target
$n$ (including pixel $i$) in the windows $t_k - \Delta - \delta t \le t \le
t_k - \Delta$ and $t_k + \Delta \le t \le t_k + \Delta + \delta t $ for
$\Delta \gg $ the timescale of the signal of interest (for example, a
transit).

We start by making the (reasonable) assumption that the observed signal---the
flux measured in pixel $i$ at time $t_k$: $y_i (t_k)$---is a function of only
the following variables:
\begin{itemize}
\item
the ``true'' flux $f_i(t_k)$ arriving at the location of pixel $i$ during the
integration time,

\item
bulk variations caused by the conditions of the spacecraft (for example,
pointing shifts and temperature fluctuations) that affect every pixel in a
systematic---but not identical---way, and

\item
instrumental noise due to the detector.
\end{itemize}
The goal is to find (or model) the true fluxes $f_i(t_k)$ conditioned on the
observations $y_i(t_k)$.
In practice, this is hard because we don't have a physical model accurate at
the required levels.
Instead, we propose an effective model based on the observations described
above.

The efficaciousness of our method relies on the following assumptions:
\begin{itemize}
\item
\emph{Independence}:
we assume that the true fluxes are independent: $f_i(t_k) \independent
f_{i^\prime} (t_{k^\prime})$ for all $i \ne i^\prime$.
We also assume there may be additional observables (for example, temperature)
for which this independence holds.

\item
\emph{Joint confounding}:
we assume that any confounding effect on $y_i(t_k)$ will also affect a number
of other values $y_{i^\prime} (t_{k^\prime})$ (where $i \ne i^\prime$ and
$t_{k^\prime}$ is not necessarily equal to $t_k$).
This means that the overall systematic affecting $y_i(t_j)$ can (in
principle) be predicted from from the other $y_{i^\prime}(t_{k^\prime})$.
\end{itemize}
To motivate our independence assumption, we refer to Reichenbach's principle
\citep{Reichenbach1956}.
It states that whenever we find two observables to be statistically dependent,
then there must be an underlying causal structure responsible for these
dependences.
This structure can be a direct causal link, or an unobserved confounder
affecting both observables.
Vice versa, if there is no causal structure connecting both observables, they
must be statistically independent.

The idea behind our approach is that if we now try to predict a measurement
$y_i (t_k)$ from variables known to be \emph{independent} of $f_i (t_k)$ (the
true flux), then whatever we \emph{can} predict has nothing to do with
$f_i (t_k)$.
Whatever is predictable better than chance---whatever statistical dependence
we find---must, by Reichenbach's principle, be due to joint confounders that
affect both our measurement and the other variables we are using to predict
it.

For simplicity, we assume that the systematic confounders affect the signal
additively but it may be possible to generalize the method.
Under this assumption, the model can be written as
\begin{eqnarray}
y_i(t_k) = f_i (t_k) + \sum_{i^\prime,\,k^\prime} c_{i^\prime\,k^\prime}\,
    y_{i^\prime} (t_{k^\prime}) \quad.
\end{eqnarray}
We propose to estimate the vector of coefficients $\mathbf{c}$ by training a
regression model on the values $y_{i^\prime} (t_{k^\prime})$.
In this model, the true flux $f_i (t_k)$ is considered ``noise''---even though
it the main signal of interest.
This works because (as described above) the signal is independent of the
functions $y_{i^\prime} (t_{k^\prime})$ being used to build the model.

\paragraph{Autoregressive generalization}
The model discussed in the previous section should retain \emph{any true
astrophysical signals} because there is no physical mechanism that would
induce correlations between these signals in different targets.
Suppose that we are only interested in temporally compact astrophysical
signals---exoplanet transits, for example.
In this case, we can substantially improve the predictive power of the model
by using information from the (recent) past of the pixels targeting the star
that we are trying to model.
In this context, a weaker version of the independence assumptions still
applies.
The signal is independent of the data targeting \emph{other stars} and the
observations targeting the target of interest for times $t_{k^\prime}$ where
$|t_k - t_{k^\prime}| > \Delta$ where $\Delta$ is longer than the maximum
duration of a transit.
This independence assumption also has a physical interpretation: in addition
to all the above independences, the star's light emission is independent of
where the planet is located around the star.
Therefore, the light arriving from the star system contains no information
about the transit signal except while the planet is in front of (or behind)
the star.
This can be viewed as an assumption of independence of the stellar limb
darkening profile and the geometric mechanism by which the
star-planet system ``converts'' the star brightness into an observable
brightness of the joint system.
This kind of assumption has recently studied in the field of causal inference
by \citet{JanSch10}.

\subsection{Deep learning}\label{sec:deep}

DFM: HERE'S WHERE I WOULD PUT THE FERGUS TEXT.

\section{Physical modeling}\label{sec:physical}

Data-driven models of the types described above can work
  because the relationships between the different data flowing from the system
  (telemetered pixel values, in this case)
  are set by an underlying set of physical devices
  (s/c, optics, detectors, and electronics, in this case),
  each of which is deterministic or stationary or at least predictable probabilistically.
Data-driven models have the advantage of flexibility%
  ---we don't have to decide in advance exactly what freedoms to give the model---%
  but they have the disadvantage that they don't represent all of our prior beliefs
  about what \emph{can} happen and why.
For these reasons, it is worth thinking about physical models.
In a physical model, we build highly parameterized models
  of the s/c attitude, PSF, flat-field (and possibly intrapixel sensitivity),
  and DC signals.
These are compared with the data probabilistically,
  through a likelihood function.
For the purposes of demonstrating the physical modeling direction,
  in this part of the \documentname\ we use simulated \Kepler\ data,
  to demonstrate feasibility.
As with the previous \sectionname, the demonstrations and discussions here are designed
  to be \emph{illustrative};
  we have not yet proven that 10~ppm precision is straightforwardly obtained by these methods.

\subsection{Toy simulation and modeling}\label{sec:toys}

Motivated by the \emph{371-roll scenario}%
  \footnote{See the \textit{Explanatory Appendix to the Kepler Project
      call for white papers:\ Kepler 2-Wheel Pointing Control}
    (\url{http://keplergo.arc.nasa.gov/docs/Kepler-2-Wheel-pointing-performance.pdf}).}
  we produced a series of toy (that is, unrealistic in various ways)
  simulated 30-minute coadd images with $\sim$ half-pixel drift and 1-arcsec jitter.
Drift and jitter were simulated by moving the boresight of the simulated telescope
  every 6~seconds during the integration.
We examine an aperture around a target,
  where the aperture is large enough to contain the full drift path
  of the target over the 24-hour observing window.
The input (true) positions of the stars in the simulation are
  taken from the \Kepler\ Input Catalog.
The input (true) PSF is a very much simplified double-Gaussian.
We introduced variations in the flat-field,
   drawn pixelwise iid from a Gaussian with 1~percent standard deviation.
We produced a series of 48 coadds,
   corresponding to one day's observations between rolls.

In order to simplify the modeling task,
  in addition to the science exposure
  we rendered a ``perfect'', bright, isolated, noise-free star with an unperturbed flat-field.
The star is drifted and jittered in each coadd identically with the full science field.
We use this ideal star to learn the PSF model in each coadd.
For a PSF model we use a very simple mixture of 4 Gaussians.
That is \emph{we used a different model to generate the data than we used to fit the data},
  which is conservative and realistic in this context.
The fake PSF-determination data and a model of it are visualized in \figurename~\ref{fig:toypsf}.

After learning a PSF model for each image,
  we infer the brightness of each source in the field.
We assume we have the true source positions from the \Kepler\ Input Catalog
  (the same positions used to produce the simulations),
  and the multi-Gaussian model of the drift-path-wrecked PSF.
This model is unrealistic because we have only considered a single PSF and only two-dimensional
  field shifts in generating the PSF;
  in reality there would be a model of the PSF over the field
  and three degrees of freedom to the s/c drift model (pointing and orientation).
The toy data and models are visualized in  \figurename~\ref{fig:toydata}.

After fitting for the brightness of each source,
   we estimate the flat-field.
We do this in a very simple way;
  we take the geometric mean of the ratio of the model to simulated images.
(This method is overly naive; we could do better in principle with simultaneous modeling.)
The true and recovered flat-fields are shown in \figurename~\ref{fig:toyflat}.
We do a good job of recovering the flat-field...HOGG

\subsection{Inferring an intrapixel flat-field}\label{sec:intrapixel}

Next, we tested whether there is any chance within these toy simulations
  that we could recover a sub-pixel sensitivity map;
  that is, is it possible to constrain the flat-field at an angular resolution
  higher than the read-out data?
The simulations are similar to the above,
  except this time we produced the simulated images on a pixel grid of twice the true resolution,
  and randomly generated a flat-field with this same resolution.
After scaling the double-resolution by the double-resolution flat-field,
  we binned down (summed) $2\times 2$ to produce images at the detector read-out resolution,
  but now with sub-pixel variations in the sensitivity.
In this simple version then, each pixel has 4 ``zones'' of different sensitivity.

Using our modeling framework,
  it is straightforward to infer such a double-resolution flat-field
  given the normal-resolution images.
As when producing the simulations,
  we render the model images at twice the resolution and scale by a model (parameterized) flat-field
  (that has four times as many parameters as there are device pixels)
  before binning and comparing with the observed images.
As in the previous experiment,
  we first solve for a PSF model,
  then solve for the brightness of each source,
  then solve for the (double-resolution) flat field.
That is, we stay in the fit--fit--fit mode rather than performing simultaneous inference
  of the three components, which should in the end perform better.
We determine the best-fit flat-field by iteratively solving a linearized least-squares problem:
Starting at a constant flat-field,
  we compute changes to the flat-field pixels that will minimize the difference
  between the model (predicted) images and the observed images.
As before, we fit simultaneously a stack of 48 images, each a 30-minute coadd.
In this experiment, the stars drift horizontally across the field,
  and some regions do not see many stars during the exposure.
To avoid numerical overfitting, we add a prior on the flat-field values,
with the same variance as the distribution from which the true flat-field was drawn ($\sigma = 2$~percent).

The results---shown in \figurename~\ref{fig:intrapixel}---%
  are that we can successfully recover a sub-pixel flat-field (sensitivity map),
  especially in regions that are jointly constrained by having stars drift through.
In other words, the quality of the intrapixel flat recovery is a strong function of the history
  of detector illumination, but it is better where we need it to be better (on the stars).
This experiment used only 48 exposures,
  while in practice we would likely attempt to infer a finer sensitivity map
  over a much larger number of exposures: likely months or more of observations.
The degraded pointing and drift of Kepler in two-wheel mode is actually a boon here,
  since it increases the diversity of stars that touch different combinations of pixels.

These experiments justify great optimism about the prospects for modeling the \Kepler\ data in the 2-wheel era.

\subsection{PSF modeling}\label{sec:psf}
\begin{itemize}
\item Drift will cause the PSF to vary from exposure to exposure due
  to the random nature of the jitter introduced by pointing
  inaccuracy. While previously due to stable pointing the jitter
  contributed to the Kepler's optical PSF in form of an enlarged PSF,
  in the two-wheel era jitter will render the PSF time-varying and
  to change from exposure to exposure. Therefore, we argue that
  precise PSF modeling is required on an individual exposure level to
  maintain high-precision photometry.

\item The PSF in Kepler's two-wheel era can be decomposed in two
  distinct components: one that describes Kepler's (unchanged) system
  PSF and a second that describes the global motion/drift of the
  spacecraft during exposure. 

  For modeling the former, previous Kepler data is invaluable, which
  will tie up on the work of \cite{bryson2010} and allow a precise
  model of Kepler's instrument PSF with the help of a detailed
  analysis of the temporal stability of Kepler's pixel response
  function (PRF). While in \cite{bryson2010} isolated non-saturated
  star observations during commissioning have been used to build a
  model of Kepler's PRF as a piecewise-continuous polynomial on a
  sub-pixel mesh, in principle any non-saturated star images from the
  Kepler Input Catalog can be used. We argue that more data will in
  general facilitate the estimation and at the same time improve
  modeling accuracy.

  The second PSF component stemming from spacecraft motion/drift is
  affecting the image in a globally consistent way. Hence, with
  precise knowledge of the detector's geometry and its possibly time
  and space-varying PRF, all star images from an individual exposure
  will help to constrain this (to some extent unkown) motion. Any
  additional knowledge about the spacecraft's pointing position will
  facilitate the estimation procedure. Note, that for an individual
  exposure we can be agnostic about the chronological history and
  temporal evolution, as we are interested in its time integrated
  effect on the image only. The incorporation of a geometric model
  allowing for physically plausible motion only has been applied with
  great success to the problem of camera shake removal with
  non-stationary blur within the computational photography and
  computer vision community (\cite{whyte2010,hirsch2011}).

\item For PSF modeling we explore two alternative, yet complimentary
  strategies: parametric and non-parametric description of the PSF.

\item It hasn't been shown so far that precise PSF modeling does
  improve photometry. What do we gain over simple aperture averaging
  and why? Can we backup our claim by references?

\end{itemize}

\section{Changes to operations and telemetry}\label{sec:operations}

need to reduce targets?

need to make apertures bigger or real-time adaptive?

need to telemeter down more housekeeping data?

need to do more active and frequent work with the jets?

\section{Calibration strategy}\label{sec:calibration}

Will self-cal be enough?

Should we pull focus and dither and exercise exposure time options?  Yes.

\section{Possible showstoppers}\label{sec:stop}

In principle any physical effect can be covered by the modeling
  methods we propose.
Since the well pointed \Kepler\ was good (with lots of post-processing)
  at the $10^{-5}$ level,
  the badly pointed must also be predictable at this level.  No?

Heterogeneity or time dependence in intrapixel sensitivity variations
could be very difficult to model.

Propose multipole expansion to begin to address this.

\section{Acknowledgments}

It is a pleasure to thank Sameer Agarwal (Google) and Keir Mierle
  (Locu) for releasing the \project{Ceres} non-linear least squares
  solver (\url{https://code.google.com/p/ceres-solver/}) and for making
  suggestions that enabled this work.
DWH, RF, and DFM are all partially supported by NSF grant IIS-1124794.
MH acknowledges support from the European Research Council in the form of a Starting Grant with number 240672.
BTM is supported by the NSF Graduate Research Fellowship grant DGE-1144469.

% ---
% despite what astronomers say, let's put titles in the references
% ---
\begin{thebibliography}{}\raggedright%

\bibitem[Foreman-Mackey \etal(2013)]{emcee}
Foreman-Mackey,~D., Hogg,~D.~W., Lang,~D., \& Goodman,~J., 2013,
\emcee:\ The MCMC Hammer,
\pasp, 125, 306--312.

\bibitem[Hogg \& Lang(2013)]{hoggtractor}
Hogg,~D.~W. \& Lang,~D., 2013,
Replacing standard galaxy profiles with mixtures of Gaussians,
\pasp, 125, 719--730

\bibitem[Lang \& Hogg(2014)]{langtractor}
Lang,~D. \& Hogg,~D.~W., 2014,
\TheTractor:\ A framework for image modeling, with applications to \SDSS, \WISE, and \Kepler,
in preparation

\bibitem[Janzing \& Sch{\"o}lkopf(2010)]{JanSch10}
Janzing,~D.\ \& Sch{\"o}lkopf,~B., 2010,
% DFM NEED TITLE
IEEE Transactions on Information
Theory, 56, 5168

\bibitem[Reichenbach(1956)]{Reichenbach1956}
Reichenbach,~H., \emph{The Direction of Time}, University of California
Press, 1956

\bibitem[Smith \etal(2012)]{map-pdc2}
Smith, J.~C., Stumpe, M.~C., Van Cleve, J.~E., \etal\ 2012, \pasp, 124, 1000

\bibitem[Stumpe \etal(2012)]{map-pdc1}
Stumpe, M.~C., Smith, J.~C., Van Cleve, J.~E., \etal\ 2012, \pasp, 124, 985

\bibitem[Bryson \etal(2010)]{bryson2010}
Bryson, S.~T., The Kepler Pixel Response Function, The Astrophysical Journal Letters 713 (2010) L97. 

\bibitem[Whyte \etal(2010)]{whyte2010} Whyte, O., Sivic, J.,
  Zissermann, A., Ponce, J., Non-uniform Deblurring for Shaken Images,
  Proceedings of the IEEE International Conference on Computer Vision
  and Pattern Recognition (CVPR) 2010.

\bibitem[Hirsch \etal(2011)]{hirsch2011} Hirsch, M., Schuler, C.~J.,
  Sch\"olkopf, B., Harmeling, S., Fast Removal of Non-uniform camera
  shake, Proceedings of the IEEE International Conference on Computer
  Vision (ICCV) 2011.

\bibitem[Fergus \etal(2006)]{fergus2006} Fergus, R., Singh, B.,
  Hertzmann, A., Roweis, S.~T., Freeman, W.~T., Removing camera shake
  from a single image, ACM Transactions on Graphics (SIGGRAPH).
\end{thebibliography}

\clearpage
\begin{figure}
\includegraphics[width=0.32\textwidth]{sim-100.png}%
\includegraphics[width=0.32\textwidth]{sim-101.png}%
\includegraphics[width=0.32\textwidth]{sim-102.png}
\caption{Toy simulated 2-wheel \Kepler\ PSF-determination data and models.
\textsl{(left)}~``Perfect'' (that is, extremely high signal-to-noise) PSF simulated image, shown on a log scale with a dynamic range of $10^6$.  These simulations were made with a simple PSF model but then 371-roll-level drift and jitter in s/c pointing.
\textsl{(middle)}~Model maximum-likelihood PSFs.  These are 4-component Gaussian mixtures (23 parameters total); that is, the model used to fit the PSF lives in a different space than the model used to generate the toy data.
\textsl{(right)} PSF-model residuals.  The stretch is $\pm 10^{-4}$.\label{fig:toypsf}}
\end{figure}

\begin{figure}
\includegraphics[width=0.49\textwidth]{sim-103.png}%
\includegraphics[width=0.49\textwidth]{sim-104.png}
\caption{Toy simulated 2-wheel simulated science data \textsl{(left)} and models \textsl{(right)}.
The model images are created by fitting the amplitude of a (rigidly fit) PSF model for each image,
  and thereby inferring the brightness of each source.
The three images shown are the first, middle, and last images
  in a 48-image sequence in the 371-roll scenario.\label{fig:toydata}}
\end{figure}

\begin{figure}
\includegraphics[width=0.49\textwidth]{sim-105.png}%
\includegraphics[width=0.49\textwidth]{sim-106.png}
\caption{Toy simulated true flat-field \textsl{(top)} and point-estimation thereof,
  found by taking ratios of models to data after fitting the drifted, effective PSF and brightness of every source in the field.
  The true flat-field in the image has Gaussian variation with 1~percent standard deviation.
  We estimated the flat-field with the pixel-wise ratio between the model image and data.\label{fig:toyflat}}
\end{figure}

\begin{figure}
\includegraphics[width=0.49\textwidth]{sim2-103.png}%
\includegraphics[width=0.49\textwidth]{sim2-200a.png}\\
\includegraphics[width=0.49\textwidth]{sim2-200.png}%
\includegraphics[width=0.49\textwidth]{sim2-201.png}\\
\includegraphics[width=0.49\textwidth]{sim2-202.png}
\caption{Demonstration that we can fit the intrapixel flat, at least in principle.
\textsl{(top two b/w images)}~Three toy images from the simulated data with $2\times 2$ sub-pixel 2-percent flat-field variations,
  and the coadd of all 48 toy images, showing the 371-roll drift and the coverage of this field patch by stars.
\textsl{(bottom three color images)}~True sub-pixel flat-field, estimated sub-pixel flat-field, and residuals.
  These objects have four times as many parameters as pixels in each of the 48 individual simulated data read-outs.
  Because of the regularization we added to avoid overfitting,
  regions that did not see a fairly bright star are relatively poorly constrained
  and therefore tend to keep the flat-field near unity, and tend to fit original-sized pixel blocks.
Regions through which bright stars drift are much better constained and very closely approximate the true flat field.
In practice, we would likely fit much larger series of images (months or more),
rather than the 48-exposure series used here, so performance could in principle become excellent.\label{fig:intrapixel}}
\end{figure}

\end{document}
